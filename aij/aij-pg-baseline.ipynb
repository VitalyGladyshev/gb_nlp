{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import cv2\nimport string\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm    #import tqdm\nfrom os.path import join\nfrom collections import Counter\nimport re \nimport io\nimport copy\nimport psutil\nfrom tensorflow.keras import layers\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import layers\n\nfrom tensorflow.keras.layers import Dense, LSTM,GRU, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.activations import relu, sigmoid, softmax\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cpu_stats():\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memory_use = py.memory_info()[0] / 2. ** 30\n    return 'memory GB:' + str(np.round(memory_use, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process = psutil.Process(os.getpid())\nprint('start', process.memory_info().rss)\ncpu_stats()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install editdistance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import editdistance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport tensorflow as tf\nprint('TensorFlow version:', tf.__version__)\nprint('Keras version:', keras.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls -la /kaggle/input/aij-digital-peter/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = \"/kaggle/input/aij-digital-peter/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vert_sample = \"/kaggle/input/aij-digital-peter/train/images/2_13_14.jpg\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls -la /kaggle/input/aij-digital-peter/train/images/2_13_14.jpg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#В этой папке лежат txt файлы перевода\ntrans_dir = data_path + 'train/words'\n#В этой папке лежат  jpg файлы изображений\nimage_dir = data_path + 'train/images'\n\nprint(len(os.listdir(trans_dir)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на отдельное изображение."},{"metadata":{"trusted":true},"cell_type":"code","source":"example = os.listdir(image_dir)[100]\nexample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = plt.imread(image_dir+'/'+example)\nplt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"originalImage = cv2.imread(image_dir+'/'+example)\ngrayImage = cv2.cvtColor(originalImage, cv2.COLOR_BGR2GRAY)\n  \n(thresh, blackAndWhiteImage) = cv2.threshold(grayImage, 150, 255, cv2.THRESH_BINARY)\n \nplt.imshow(blackAndWhiteImage)\n  \n# cv2.waitKey(0)\n# cv2.destroyAllWindows()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(originalImage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(grayImage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = plt.imread(vert_sample)\nplt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.rotate(img, cv2.cv2.ROTATE_90_COUNTERCLOCKWISE)   # ROTATE_90_CLOCKWISE\nplt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"И его перевод."},{"metadata":{"trusted":true},"cell_type":"code","source":"with io.open(trans_dir+'/'+example[:-3]+'txt', 'r',  encoding='utf8') as file:\n    data = file.read()\n\nprint(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"В текстах встречаются слова, содержащие английские символы.\n\nНо так как английских символов в целом не очень много, то из train-выборки можно удалить строки с ними (и обучаться только на оставшихся строках). Так мы и сделаем в текущем бейзлайне. Это сократит алфавит (и, как следствие, избавит нас от лишней шумности). \n\nПри этом надо понимать, что в тестовой выборке английские символы, скорее всего, будут присутствовать. Поэтому будут и вынужденные ошибки, связанные с нашей вольной фильтрацией обучающей выборки.\n\nВпрочем, метод распознавания всегда выбираете вы сами. \n\nКроме того, советуем внимательно изучить train-выборку глазами."},{"metadata":{"trusted":true},"cell_type":"code","source":"english = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'm', 'n' ,'o', 'p', 'r', 's', 't', 'u', 'w']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_to_labels(text):\n    return list(map(lambda x: letters.index(x), text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Следующая функция подготавливает лэйблы, то есть переведенный текст. Как уже было сказано, игнорируются примеры, содержащие английские буквы."},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_texts(image_dir,trans_dir):\n    lens = []\n    include_english = 0\n    letters = ''\n\n    lines = []\n    names = []\n    \n    all_files = os.listdir(trans_dir)\n    for filename in os.listdir(image_dir):\n        if filename[:-3]+'txt' in all_files:\n            name, ext = os.path.splitext(filename)\n            txt_filepath = join(trans_dir, name + '.txt')\n            with open(txt_filepath, 'r') as file:\n                data = file.read()\n                if len(data)==0:\n                    continue\n                if len(set(data).intersection(english))>0:\n                    continue\n\n                lines.append(data)\n                names.append(filename)\n                lens.append(len(data))\n                letters += data\n    print('Максимальная длина строки:', max(Counter(lens).keys()))\n    print('Количество строк с английскими буквами ',include_english)\n\n    return names,lines,Counter(letters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"В итоге мы получаем список изображений, соответствующий ему обработанный список строк и словарь символов."},{"metadata":{"trusted":true},"cell_type":"code","source":"names,lines,cnt = process_texts(image_dir,trans_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"letters = set(cnt.keys())\n\nletters = sorted(list(letters))\nprint('Символы train:', ' '.join(letters))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Следующая функция подгружает изображения, меняет их до необходимого размера и нормирует."},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_image(img):\n    w, h,_ = img.shape\n    \n    new_w = 128\n    new_h = int(h * (new_w / w))\n    img = cv2.resize(img, (new_h, new_w))\n    w, h,_ = img.shape\n    \n    img = img.astype('float32')\n    \n    if w < 128:\n        add_zeros = np.full((128-w, h,3), 255)\n        img = np.concatenate((img, add_zeros))\n        w, h,_ = img.shape\n    \n    if h < 1024:\n        add_zeros = np.full((w, 1024-h,3), 255)\n        img = np.concatenate((img, add_zeros), axis=1)\n        w, h,_ = img.shape\n        \n    if h > 1024 or w > 128:\n        dim = (1024,128)\n        img = cv2.resize(img, dim)\n    \n    img = cv2.subtract(255, img)\n\n    img = img / 255\n    \n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_image_g(img):\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    h, w = img.shape\n#     print(f\"w: {w} h: {h}\")\n    if h > (w*2.5):\n        img = cv2.rotate(img, cv2.cv2.ROTATE_90_COUNTERCLOCKWISE)   # ROTATE_90_CLOCKWISE\n        h, w = img.shape\n    \n    new_h = 128\n    new_w = int(w * (new_h / h))\n    img = cv2.resize(img, (new_w, new_h))\n    h, w = img.shape\n    \n    img = img.astype('float32')\n    \n    if h < 128:\n        add_zeros = np.full((128-h, w), 255)\n        img = np.concatenate((img, add_zeros))\n        h, w = img.shape\n    \n    if w < 1024:\n        add_zeros = np.full((h, 1024-w), 255)\n        img = np.concatenate((img, add_zeros), axis=1)\n        h, w = img.shape\n        \n    if w > 1024 or h > 128:\n        dim = (1024,128)\n        img = cv2.resize(img, dim)\n    \n    img = cv2.subtract(255, img)\n\n    img = img / 255\n    \n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vert_sample_2 = \"/kaggle/input/aij-digital-peter/train/images/2_13_13.jpg\"\nimg_vs = plt.imread(vert_sample_2)\nimg_vs = cv2.cvtColor(img_vs, cv2.COLOR_BGR2GRAY)\nh, w = img_vs.shape\nprint(f\"w: {w} h: {h}\")\nimg_vs = process_image_g(plt.imread(vert_sample_2))\nplt.imshow(img_vs)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_vs = plt.imread(vert_sample)\nimg_vs = cv2.cvtColor(img_vs, cv2.COLOR_BGR2GRAY)\nh, w = img_vs.shape\nprint(f\"w: {w} h: {h}\")\nimg_vs = process_image_g(plt.imread(vert_sample))\nplt.imshow(img_vs)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Подготовим обучающие и валидационные данные.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_data(lines,names,image_dir):\n    data_images = []\n    data_labels = []\n    data_input_length = []\n    data_label_length = []\n    data_original_text = []\n    data_original_image = []\n    \n    max_label_len = 0\n    for line, name in tqdm(zip(lines,names)):    # for line, name in tqdm.tqdm_notebook(zip(lines,names)):\n        img = cv2.imread(image_dir+'/'+name)\n        # original_image = copy.deepcopy(img)\n        img = process_image_g(img)    # img = process_image(img)\n        try:\n            label = text_to_labels(line)\n        except:\n            print('bad_label')\n            continue\n        data_images.append(img)\n        data_labels.append(label)\n        data_input_length.append(255)\n        data_label_length.append(len(line))\n        data_original_text.append(line)\n        # data_original_image.append(original_image)\n        \n        if len(line) > max_label_len:\n            max_label_len = len(line)\n        \n    return data_images, data_labels, data_input_length, data_label_length, \\\n            data_original_text, data_original_image, max_label_len","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Создадим обучающую и валидационную выборки."},{"metadata":{"trusted":true},"cell_type":"code","source":"lines_train = []\nnames_train = []\n\nlines_val = []\nnames_val = []\n\n\nfor num,(line, name) in enumerate(zip(lines,names)):\n    if num % 15 == 0:\n        lines_val.append(line)\n        names_val.append(name)\n    else:\n        lines_train.append(line)\n        names_train.append(name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process = psutil.Process(os.getpid())\nprint('start', process.memory_info().rss)\ncpu_stats()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images, train_labels, train_input_length, train_label_length, train_original_text, train_original_image, \\\n        train_max_label_len = generate_data(lines_train, names_train, image_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process = psutil.Process(os.getpid())\nprint('start', process.memory_info().rss)\ncpu_stats()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open('train_images.pickle', 'wb') as fl:\n#     pickle.dump(train_images, fl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_images, val_labels, val_input_length, val_label_length, val_original_text, val_original_image, \\\n        val_max_label_len = generate_data(lines_val, names_val, image_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process = psutil.Process(os.getpid())\nprint('start', process.memory_info().rss)\ncpu_stats()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open('val_images.pickle', 'wb') as fl:\n#     pickle.dump(val_images, fl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ls -la","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(val_images[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_label_len = max(train_max_label_len,val_max_label_len)\nprint(max_label_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Выведем пару обработанных изображений с переводом."},{"metadata":{"trusted":true},"cell_type":"code","source":"rnd = np.random.choice(range(len(train_images)),2)\n\nfor i in rnd:\n    print(train_original_text[i])\n    plt.imshow(train_images[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Дополним строки до максимальной длины**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_padded_label = pad_sequences(train_labels, \n                             maxlen=max_label_len, \n                             padding='post',\n                             value=len(letters))\n\nval_padded_label = pad_sequences(val_labels, \n                             maxlen=max_label_len, \n                             padding='post',\n                             value=len(letters))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_padded_label.shape, val_padded_label.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process = psutil.Process(os.getpid())\nprint('start', process.memory_info().rss)\ncpu_stats()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = np.asarray(train_images, dtype='float32')\ntrain_input_length = np.asarray(train_input_length, dtype='int16')\ntrain_label_length = np.asarray(train_label_length, dtype='int16')\n\nval_images = np.asarray(val_images, dtype='float32')\nval_input_length = np.asarray(val_input_length, dtype='int16')\nval_label_length = np.asarray(val_label_length, dtype='int16')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = train_images.reshape(train_images.shape[0], train_images.shape[1], train_images.shape[2], 1)\nval_images = val_images.reshape(val_images.shape[0], val_images.shape[1], val_images.shape[2], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process = psutil.Process(os.getpid())\nprint('start', process.memory_info().rss)\ncpu_stats()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Строим модель**"},{"metadata":{},"cell_type":"markdown","source":"Модель состоит из нескольких слоев CNN, GRU и использует ctc_loss. [Вот](https://www.youtube.com/watch?v=SAfJ6nP2rrI) хорошее видео о нем на русском."},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Input(shape=(128,1024,1))\n\nconv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\npool_1 = MaxPool2D(pool_size=(4, 2), strides=2)(conv_1)\n\nconv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\npool_2 = MaxPool2D(pool_size=(4, 2), strides=2)(conv_2)\n\nconv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n\nconv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n\npool_4 = MaxPool2D(pool_size=(4, 1),padding='same')(conv_4)\n\nconv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n\nbatch_norm_5 = BatchNormalization()(conv_5)\n\nconv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\nbatch_norm_6 = BatchNormalization()(conv_6)\npool_6 = MaxPool2D(pool_size=(4, 1),padding='same')(batch_norm_6)\n\nconv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n\nsqueezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n\nblstm_1 = Bidirectional(LSTM(1024, return_sequences=True, dropout = 0.2))(squeezed)\nblstm_2 = Bidirectional(LSTM(1024, return_sequences=True, dropout = 0.2))(blstm_1)\n\noutputs = Dense(len(letters)+1, activation = 'softmax')(blstm_2)\nact_model = Model(inputs=inputs, outputs=outputs)\n    \nthe_labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\ninput_length = Input(name='input_length', shape=[1], dtype='int16')   # input_length = Input(name='input_length', shape=[1], dtype='int64')\nlabel_length = Input(name='label_length', shape=[1], dtype='int16')   # label_length = Input(name='label_length', shape=[1], dtype='int64')\n\ndef ctc_lambda_func(args):\n    y_pred, labels, input_length, label_length = args\n\n    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n\nloss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, the_labels, input_length, label_length])\n\nmodel = Model(inputs=[inputs, the_labels, input_length, label_length], outputs=loss_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"act_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 20\nepochs = 300\n\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n\nmodel.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam', metrics=['accuracy'])\n\nos.makedirs('checkpoint', exist_ok=True)\n\nfilepath=\"checkpoint/model_lstm_1024_20.hdf5\"\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.7,\n                              patience=5, min_lr=0.00001)\n\ncheckpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')\ncallbacks_list = [checkpoint,es,reduce_lr]\n\nhistory = model.fit(x=[train_images, train_padded_label, train_input_length, train_label_length],\n                    y=np.zeros(len(train_images)),\n                    batch_size=batch_size, \n                    epochs=epochs, \n                    validation_data=([val_images, val_padded_label, val_input_length, val_label_length], [np.zeros(len(val_images))]),\n                    verbose=2,\n                    callbacks=callbacks_list)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__LSTM 256 R:__\n\nEpoch 00030: val_accuracy improved from 0.22359 to 0.22604, saving model to checkpoint/model_lstm_256.hdf5\n285/285 - 90s - loss: 0.1888 - accuracy: 0.9221 - val_loss: 12.2445 - val_accuracy: 0.2260\nEpoch 31/300\n\nEpoch 00031: val_accuracy improved from 0.22604 to 0.23342, saving model to checkpoint/model_lstm_256.hdf5\n285/285 - 90s - loss: 0.2058 - accuracy: 0.9156 - val_loss: 12.6246 - val_accuracy: 0.2334\nEpoch 00031: early stopping\n\nGRU 384: \n\nEpoch 00031: val_accuracy improved from 0.22604 to 0.23342, saving model to checkpoint/model_gru.hdf5\n285/285 - 96s - loss: 1.4023 - accuracy: 0.8642 - val_loss: 11.1329 - val_accuracy: 0.2334\nEpoch 00031: early stopping\n\nLSTM 384: \n\nEpoch 00031: val_accuracy did not improve from 0.21867\n285/285 - 100s - loss: 1.0820 - accuracy: 0.9240 - val_loss: 14.4068 - val_accuracy: 0.2162\nEpoch 00031: early stopping\n\nLSTM 384 R: \n\nEpoch 00036: val_accuracy did not improve from 0.24816\n285/285 - 100s - loss: 0.1289 - accuracy: 0.9507 - val_loss: 11.0478 - val_accuracy: 0.2408\nEpoch 00036: early stopping\n\nGRU 512 R: \n\nEpoch 00031: val_accuracy did not improve from 0.23342\n285/285 - 103s - loss: 0.2373 - accuracy: 0.9094 - val_loss: 10.6688 - val_accuracy: 0.2334\nEpoch 00031: early stopping\n\nLSTM 512 R: \n\nEpoch 00029: val_accuracy did not improve from 0.23833\n285/285 - 118s - loss: 0.1149 - accuracy: 0.9602 - val_loss: 10.4089 - val_accuracy: 0.2383\nEpoch 30/300\n\nEpoch 00030: val_accuracy did not improve from 0.23833\n285/285 - 118s - loss: 0.2184 - accuracy: 0.9075 - val_loss: 10.8032 - val_accuracy: 0.1941\nEpoch 00030: early stopping\n\nLSTM 512 R 64: \n\nEpoch 00038: val_accuracy did not improve from 0.18673\n90/90 - 78s - loss: 0.1537 - accuracy: 0.9531 - val_loss: 11.5777 - val_accuracy: 0.1794\nEpoch 00038: early stopping\n\nLSTM 768 R: \n\nEpoch 00028: val_accuracy improved from 0.25061 to 0.26290, saving model to checkpoint/model_lstm_768.hdf5\n285/285 - 140s - loss: 0.1177 - accuracy: 0.9570 - val_loss: 11.3175 - val_accuracy: 0.2629\nEpoch 29/300\n\nEpoch 00029: val_accuracy did not improve from 0.26290\n285/285 - 140s - loss: 0.0994 - accuracy: 0.9684 - val_loss: 11.7199 - val_accuracy: 0.2432\nEpoch 30/300\n\nEpoch 00030: val_accuracy did not improve from 0.26290\n285/285 - 140s - loss: 0.1107 - accuracy: 0.9617 - val_loss: 12.0675 - val_accuracy: 0.2482\nEpoch 31/300\n\nEpoch 00031: val_accuracy did not improve from 0.26290\n285/285 - 140s - loss: 0.1352 - accuracy: 0.9488 - val_loss: 12.1216 - val_accuracy: 0.2457\nEpoch 00031: early stopping\n\nLSTM 1024 R: \n\nEpoch 00036: val_accuracy improved from 0.26781 to 0.28010, saving model to checkpoint/model_lstm_1024.hdf5\n285/285 - 177s - loss: 0.0433 - accuracy: 0.9875 - val_loss: 10.8352 - val_accuracy: 0.2801\nEpoch 37/300\n\nEpoch 00037: val_accuracy did not improve from 0.28010\n285/285 - 176s - loss: 0.0332 - accuracy: 0.9942 - val_loss: 10.9423 - val_accuracy: 0.2678\nEpoch 38/300\n\nEpoch 00038: val_accuracy did not improve from 0.28010\n285/285 - 176s - loss: 0.0448 - accuracy: 0.9874 - val_loss: 11.0578 - val_accuracy: 0.2629\nEpoch 39/300\n\nEpoch 00039: val_accuracy did not improve from 0.28010\n285/285 - 176s - loss: 0.0426 - accuracy: 0.9893 - val_loss: 11.1605 - val_accuracy: 0.2383\nEpoch 00039: early stopping\n\nLSTN 1024 R 10: \n\nEpoch 00028: val_accuracy improved from 0.25061 to 0.26536, saving model to checkpoint/model_lstm_1024_10.hdf5\n570/570 - 226s - loss: 0.2756 - accuracy: 0.8738 - val_loss: 10.8429 - val_accuracy: 0.2654\nEpoch 29/300\n\nEpoch 00029: val_accuracy did not improve from 0.26536\n570/570 - 225s - loss: 0.2198 - accuracy: 0.9021 - val_loss: 11.0677 - val_accuracy: 0.2654\nEpoch 30/300\n\nEpoch 00030: val_accuracy did not improve from 0.26536\n570/570 - 226s - loss: 0.2061 - accuracy: 0.9098 - val_loss: 11.3000 - val_accuracy: 0.2408\nEpoch 31/300\n\nEpoch 00031: val_accuracy did not improve from 0.26536\n570/570 - 226s - loss: 0.1802 - accuracy: 0.9210 - val_loss: 11.4935 - val_accuracy: 0.2580\nEpoch 32/300\n\nEpoch 00032: val_accuracy improved from 0.26536 to 0.27764, saving model to checkpoint/model_lstm_1024_10.hdf5\n570/570 - 227s - loss: 0.1871 - accuracy: 0.9147 - val_loss: 11.7252 - val_accuracy: 0.2776\nEpoch 00032: early stopping\n\nLSTM 1024 R 40: \n\nEpoch 00036: val_accuracy improved from 0.26290 to 0.26536, saving model to checkpoint/model_lstm_1024_40.hdf5\n143/143 - 140s - loss: 0.0258 - accuracy: 0.9967 - val_loss: 11.9028 - val_accuracy: 0.2654\nEpoch 37/300\n\nEpoch 00037: val_accuracy did not improve from 0.26536\n143/143 - 138s - loss: 0.0218 - accuracy: 0.9979 - val_loss: 11.8603 - val_accuracy: 0.2506\nEpoch 38/300\n\nEpoch 00038: val_accuracy did not improve from 0.26536\n143/143 - 138s - loss: 0.0179 - accuracy: 0.9988 - val_loss: 11.9197 - val_accuracy: 0.2604\nEpoch 39/300\n\nEpoch 00039: val_accuracy did not improve from 0.26536\n143/143 - 138s - loss: 0.0168 - accuracy: 0.9988 - val_loss: 11.9836 - val_accuracy: 0.2629\nEpoch 00039: early stopping\n\n\nLSTM 1024 R 20: \n\nEpoch 00032: val_accuracy improved from 0.28010 to 0.28501, saving model to checkpoint/model_lstm_1024_20.hdf5\n285/285 - 179s - loss: 0.0981 - accuracy: 0.9658 - val_loss: 11.8005 - val_accuracy: 0.2850\nEpoch 33/300\n\nEpoch 00033: val_accuracy did not improve from 0.28501\n285/285 - 177s - loss: 0.1213 - accuracy: 0.9605 - val_loss: 13.4844 - val_accuracy: 0.2506\nEpoch 34/300\n\nEpoch 00034: val_accuracy did not improve from 0.28501\n285/285 - 178s - loss: 0.1504 - accuracy: 0.9403 - val_loss: 11.9105 - val_accuracy: 0.2826\nEpoch 00034: early stopping"},{"metadata":{},"cell_type":"markdown","source":"Посмотрим качество на валидации."},{"metadata":{"trusted":true},"cell_type":"code","source":"import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nact_model.load_weights(filepath)\nprediction = act_model.predict(val_images)\n\ndecoded = K.ctc_decode(prediction, \n                       input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n                       greedy=True)[0][0]\nout = K.get_value(decoded)\nend = time.time()\nprint(end-start)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM 256 R: 4.716141700744629\n\nLSTM 384: 5.0529961585998535\n\nLSTM 384 R: 4.932627439498901\n\nGRU 512 R: 4.939647436141968\n\nLSTM 512 R: 5.386308431625366\n\nLSTM 512 R 64: 4.418805360794067\n\nLSTM 768 R: 5.505225658416748\n\nLSTM 1024 R: 6.45198130607605\n\nLSTM 1024 R 10: 6.250160455703735\n\nLSTM 1024 R 40: 6.376370429992676\n\nLSTM 1024 R 20: 6.619359016418457"},{"metadata":{"trusted":true},"cell_type":"code","source":"numCharErr = 0\nnumCharTotal = 0\nnumStringOK = 0\nnumStringTotal = 0\n\nword_eds, word_true_lens = [], []\n\nprint('Ground truth -> Recognized')\t\nfor i in range(len(out)):\n    pred = ''\n    for p in out[i]:\n        if int(p) != -1:\n            pred+=letters[int(p)]\n    true = val_original_text[i]\n    \n    numStringOK += 1 if true == pred else 0\n    numStringTotal += 1\n    dist = editdistance.eval(pred, true)\n    numCharErr += dist\n    numCharTotal += len(true)\n    \n    pred_words = pred.split()\n    true_words = true.split()\n    word_eds.append(editdistance.eval(pred_words, true_words))\n    word_true_lens.append(len(true_words))\n    \n    print('[OK]' if dist==0 else '[ERR:%d]' % dist,'\"' + true + '\"', '->', '\"' + pred + '\"')\n\ncharErrorRate = numCharErr / numCharTotal\nwordErrorRate = sum(word_eds) / sum(word_true_lens) \nstringAccuracy = numStringOK / numStringTotal\nprint('Character error rate: %f%%. Word error rate: %f%%. String accuracy: %f%%.' % \\\n      (charErrorRate*100.0,wordErrorRate*100.0, stringAccuracy*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GRU 256: Character error rate: 9.502422%. Word error rate: 41.184637%. String accuracy: 24.078624%.\n\nLSTM 256 R: Character error rate: 7.601966%. Word error rate: 40.105163%. String accuracy: 27.764128%.\n\nGRU 384: Character error rate: 7.710172%. Word error rate: 38.246817%. String accuracy: 28.501229%.\n\nLSTM 384: Character error rate: 9.379128%. Word error rate: 39.194817%. String accuracy: 27.027027%.\n\nLSTM 384 R: Character error rate: 6.605020%. Word error rate: 36.094401%. String accuracy: 28.992629%.\n\nGRU 512 R: Character error rate: 6.754734%. Word error rate: 35.585377%. String accuracy: 29.238329%.\n\nLSTM 512 R: Character error rate: 6.428240%. Word error rate: 35.210578%. String accuracy: 28.992629%.\n\nLSTM 512 R 64: Character error rate: 8.026010%. Word error rate: 42.115573%. String accuracy: 22.358722%.\n\nLSTM 768 R: Character error rate: 6.382010%. Word error rate: 33.652008%. String accuracy: 30.712531%.\n\nLSTM 1024 R: Character error rate: 6.372503%. Word error rate: 33.496572%. String accuracy: 30.958231%.\n\nLSTM 1024 R 10: Character error rate: 6.636926%. Word error rate: 34.751434%. String accuracy: 33.906634%.\n\nLSTM 1024 R 40: Character error rate: 6.882890%. Word error rate: 36.076555%. String accuracy: 29.484029%.\n\nLSTM 1024 R 20: Character error rate: 6.454843%. Word error rate: 34.703633%. String accuracy: 33.415233%."},{"metadata":{},"cell_type":"markdown","source":"**Загружаем тестовые изображения, делаем предсказания и записываем их в файл.**"},{"metadata":{},"cell_type":"markdown","source":"Этот ноутбук - публичный бейзлайн. Тестовая выборка будет автоматически подкладываться в образ, куда участник загрузит модель. Поэтому сейчас следующие строчки закомменчены. И пример запуска модели и записи предсказаний в файл будет проведен на валидационных изображениях."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Для примера работы запуска модели и записи в файл, пусть валидация станет тестовыми изображениями\ntest_images = val_images\nnames_test = names_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_image_dir = 'test/images'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_images = []\n# names_test = []\n\n# for name in  os.listdir(test_image_dir):\n#     img = cv2.imread(test_image_dir+'/'+name)\n#     img = process_image(img)\n#     test_images.append(img)\n#     names_test.append(name)\n# test_images = np.asarray(test_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n\nprediction =act_model.predict(test_images)\n \ndecoded = K.ctc_decode(prediction,   \n                       input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n                       greedy=True)[0][0]\n\nout = K.get_value(decoded)\n\nprediction = []\nfor i, x in enumerate(out):\n    pred = ''\n    for p in x:\n        if int(p) != -1:\n            pred+=letters[int(p)]\n            \n    prediction.append(pred)\nend = time.time()\nprint(end - start)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM 256 R: 1.8709514141082764\n\nGRU 384: 1.6304709911346436\n\nGRU 384: 1.8490509986877441\n\nLSTM 384: 1.880892038345337\n\nLSTM 384 R: 1.93514084815979\n\nGRU 512 R: 2.5451066493988037\n\nLSTM 512 R: 2.2609000205993652\n\nLSTM 512 R 64: 2.2071831226348877\n\nLSTM 768 R: 2.57204532623291\n\nLSTM 1024 R: 3.180837631225586\n\nLSTM 1024 R 10: 3.1426846981048584\n\nLSTM 1024 R 40: 3.4323489665985107\n\nLSTM 1024 R 20: 3.10937762260437"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_dir = 'predictions'\nos.makedirs(output_dir, exist_ok=True)\n\nfor num, (name, line) in enumerate(zip(names_test, prediction)):\n    with open(os.path.join(output_dir, name.replace('.jpg', '.txt')), 'w') as file:\n        file.write(line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls -la ./checkpoint/","execution_count":72,"outputs":[{"output_type":"stream","text":"total 509256\r\ndrwxr-xr-x 2 root root      4096 Nov 11 11:50 \u001b[0m\u001b[01;34m.\u001b[0m/\r\ndrwxr-xr-x 4 root root      4096 Nov 11 13:51 \u001b[01;34m..\u001b[0m/\r\n-rw-r--r-- 1 root root 521462444 Nov 11 13:23 model_lstm_1024_20.hdf5\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'./checkpoint/model_lstm_1024_20.hdf5')","execution_count":73,"outputs":[{"output_type":"execute_result","execution_count":73,"data":{"text/plain":"/kaggle/working/checkpoint/model_lstm_1024_20.hdf5","text/html":"<a href='./checkpoint/model_lstm_1024_20.hdf5' target='_blank'>./checkpoint/model_lstm_1024_20.hdf5</a><br>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cat ./predictions/106_1_14.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import sys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ls -la","execution_count":71,"outputs":[{"output_type":"stream","text":"total 475300\r\ndrwxr-xr-x 4 root root      4096 Nov 11 13:51 \u001b[0m\u001b[01;34m.\u001b[0m/\r\ndrwxr-xr-x 5 root root      4096 Nov 11 11:45 \u001b[01;34m..\u001b[0m/\r\n---------- 1 root root       263 Nov 11 11:45 __notebook_source__.ipynb\r\ndrwxr-xr-x 2 root root      4096 Nov 11 11:50 \u001b[01;34mcheckpoint\u001b[0m/\r\ndrwxr-xr-x 2 root root     20480 Nov 11 13:29 \u001b[01;34mpredictions\u001b[0m/\r\n-rw-r--r-- 1 root root 486664995 Nov 11 13:51 submit.zip\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ! zip submit.zip -r ./checkpoint/model_lstm_1024_20.hdf5","execution_count":70,"outputs":[{"output_type":"stream","text":"  adding: checkpoint/model_lstm_1024_20.hdf5 (deflated 7%)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def read(path, names=None):\n#     if names is None:\n#         names = sorted(os.listdir(path))\n#     else:\n#         names_presented = sorted(os.listdir(path))\n#         if names_presented != names:\n#             sys.exit('The list of recognized files does not match the complete list of files with the correct transcription!')\n\n#     data = []\n#     for name in names:\n#         with open(os.path.join(path, name), 'r') as file:\n#             data.append(file.read().strip())\n            \n#     return names, data\n\n\n# # Это скрипт который посчитает 3 метрики качества, по которым оценивается решение\n# def evaluate(true_path, pred_path):\n#     # Аргументы - названия файлов с предсказаниями и правильными ответами.\n#     # В true записаны построчно правильные ответы. В pred - соответствующие предсказания\n# #     pred_path = sys.argv[1]\n# #     true_path = sys.argv[2]\n\n#     names, true_text = read(true_path)\n#     _, predictions = read(pred_path, names)\n\n#     # По сути, мы в цикле пробегаемся по предсказаниям, считаем расстояние Левенштейна,\n#     # а затем делим сумму расстояний на сумму длин всех правильных ответов\n#     def cer():\n#         numCharErr = 0\n#         numCharTotal = 0\n\n#         for i in range(len(predictions)):\n#             pred = predictions[i]\n#             true = true_text[i]\n#             dist = editdistance.eval(pred, true)\n#             numCharErr += dist\n#             numCharTotal += len(true)\n#         charErrorRate = numCharErr / numCharTotal\n#         return charErrorRate * 100\n\n#     # Аналогичный подход, как и для CER. Только объектом является слово, а не символ.\n#     # Соответственно в подсчете расстояния участвует два массива - pred и true, в массивах содержатся слова\n#     def wer():\n#         word_eds, word_true_lens = [], []\n#         for i in range(len(predictions)):\n#             pred = predictions[i]\n#             true = true_text[i]\n\n#             pred_words = pred.split()\n#             true_words = true.split()\n#             word_eds.append(editdistance.eval(pred_words, true_words))\n#             word_true_lens.append(len(true_words))\n\n#         wordErrorRate = sum(word_eds) / sum(word_true_lens)\n#         return wordErrorRate * 100\n\n#     # Наиболее простая метрика, которая считает количество полных совпадений предложений (pred и true)\n#     # и делит его на общий размер теста\n#     def string_acc():\n#         numStringOK = 0\n#         numStringTotal = 0\n\n#         for i in range(len(predictions)):\n#             pred = predictions[i]\n#             true = true_text[i]\n\n#             numStringOK += 1 if true == pred else 0\n#             numStringTotal += 1\n\n#         stringAccuracy = numStringOK / numStringTotal\n\n#         return stringAccuracy * 100\n\n#     # Вызов функций и подсчет метрик\n#     charErrorRate = cer()\n#     wordErrorRate = wer()\n#     stringAccuracy = string_acc()\n\n#     # Выведем построчно правильные ответы и предсказания\n#     print('Ground truth -> Recognized')\n#     for i in range(len(predictions)):\n#         pred = predictions[i]\n#         true = true_text[i]\n\n#         dist = editdistance.eval(pred, true)\n\n#         print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + true + '\"', '->', '\"' + pred + '\"')\n\n#     print('Character error rate: %f%%' % charErrorRate)\n#     print('Word error rate: %f%%' % wordErrorRate)\n#     print('String accuracy: %f%%' % stringAccuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate(true_path, ./predictions/)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Соберем сабмит для платформы"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ! cp checkpoint/model.hdf5 submit_example/checkpoint/\n# ! zip submit.zip -r submit_example","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}