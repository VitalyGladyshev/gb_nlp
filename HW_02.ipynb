{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlED1yfIPg5u5oUpGax9zx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VitalyGladyshev/gb_nlp/blob/main/HW_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc-e3zAz75Nb"
      },
      "source": [
        "# ДЗ 02 Пока заглушка. Скоро сделаю и сдам..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDhhr_EdDCQ9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEjzCbVxGJ_5",
        "outputId": "42cdd6ba-bb82-4a08-8644-a94e495b4bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxdk_PfPGR9_"
      },
      "source": [
        "path_nlp = \"/content/gdrive/My Drive/Colab Notebooks/NLP/\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtLG-QX4FeYG"
      },
      "source": [
        "train_cor = pd.read_pickle(path_nlp + \"train_cor.pkl\")\n",
        "test_cor = pd.read_pickle(path_nlp + \"test_cor.pkl\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-cHtRE-Gr_o",
        "outputId": "c5656b15-7ab7-467e-dc9a-5ffe0b5bfd31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "train_cor.tail(2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>clean_tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>tweet_token_filtered</th>\n",
              "      <th>tweet_stemmed</th>\n",
              "      <th>tweet_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31960</th>\n",
              "      <td>31961</td>\n",
              "      <td>1</td>\n",
              "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
              "      <td>sikh temple vandalised in in calgary wso conde...</td>\n",
              "      <td>[sikh, temple, vandalised, in, in, calgary, ws...</td>\n",
              "      <td>[sikh, temple, vandalised, calgary, wso, conde...</td>\n",
              "      <td>{vandalis, wso, templ, act, condemn, sikh, cal...</td>\n",
              "      <td>{vandalise, wso, calgary, act, temple, sikh, c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31961</th>\n",
              "      <td>31962</td>\n",
              "      <td>0</td>\n",
              "      <td>thank you @user for you follow</td>\n",
              "      <td>thank you for you follow</td>\n",
              "      <td>[thank, you, for, you, follow]</td>\n",
              "      <td>[thank, follow]</td>\n",
              "      <td>{thank, follow}</td>\n",
              "      <td>{thank, follow}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                   tweet_lemmatized\n",
              "31960  31961  ...  {vandalise, wso, calgary, act, temple, sikh, c...\n",
              "31961  31962  ...                                    {thank, follow}\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2qwUjUGxLK",
        "outputId": "11aa00ba-fdd1-4cac-b39a-e791b02c50c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "test_cor.tail(2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>clean_tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>tweet_token_filtered</th>\n",
              "      <th>tweet_stemmed</th>\n",
              "      <th>tweet_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49157</th>\n",
              "      <td>49158</td>\n",
              "      <td>happy, at work conference: right mindset leads...</td>\n",
              "      <td>happy at work conference right mindset leads t...</td>\n",
              "      <td>[happy, at, work, conference, right, mindset, ...</td>\n",
              "      <td>[happy, work, conference, right, mindset, lead...</td>\n",
              "      <td>{lead, confer, organ, mindset, right, work, cu...</td>\n",
              "      <td>{conference, lead, cultureofdevelopment, happy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49158</th>\n",
              "      <td>49159</td>\n",
              "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
              "      <td>my song so glad free download shoegaze newmusi...</td>\n",
              "      <td>[my, song, so, glad, free, download, shoegaze,...</td>\n",
              "      <td>[song, glad, free, download, shoegaze, newmusi...</td>\n",
              "      <td>{shoegaz, newsong, download, glad, newmus, fre...</td>\n",
              "      <td>{newsong, download, glad, free, newmusic, shoe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                   tweet_lemmatized\n",
              "49157  49158  ...  {conference, lead, cultureofdevelopment, happy...\n",
              "49158  49159  ...  {newsong, download, glad, free, newmusic, shoe...\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j34p8UCYKVpJ"
      },
      "source": [
        "test_cor['label'] = -1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUNiAYnUKbz5",
        "outputId": "3c018c72-2209-4078-b1fd-390f34e52003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "data = train_cor.copy()\n",
        "data = data.append(test_cor, ignore_index=True)\n",
        "data.tail(2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>clean_tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>tweet_token_filtered</th>\n",
              "      <th>tweet_stemmed</th>\n",
              "      <th>tweet_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49157</th>\n",
              "      <td>49158</td>\n",
              "      <td>-1</td>\n",
              "      <td>happy, at work conference: right mindset leads...</td>\n",
              "      <td>happy at work conference right mindset leads t...</td>\n",
              "      <td>[happy, at, work, conference, right, mindset, ...</td>\n",
              "      <td>[happy, work, conference, right, mindset, lead...</td>\n",
              "      <td>{lead, confer, organ, mindset, right, work, cu...</td>\n",
              "      <td>{conference, lead, cultureofdevelopment, happy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49158</th>\n",
              "      <td>49159</td>\n",
              "      <td>-1</td>\n",
              "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
              "      <td>my song so glad free download shoegaze newmusi...</td>\n",
              "      <td>[my, song, so, glad, free, download, shoegaze,...</td>\n",
              "      <td>[song, glad, free, download, shoegaze, newmusi...</td>\n",
              "      <td>{shoegaz, newsong, download, glad, newmus, fre...</td>\n",
              "      <td>{newsong, download, glad, free, newmusic, shoe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                   tweet_lemmatized\n",
              "49157  49158  ...  {conference, lead, cultureofdevelopment, happy...\n",
              "49158  49159  ...  {newsong, download, glad, free, newmusic, shoe...\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBYrXBu1AtTP"
      },
      "source": [
        "## Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p6I9WjABESp"
      },
      "source": [
        "Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
        "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
        "- Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
        "- Исключим стоп-слова с помощью stop_words='english'.\n",
        "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names().\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQS2EMSf7ysg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FNwHKswAzxO"
      },
      "source": [
        "## Задание 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGSkgAooBX-G"
      },
      "source": [
        "Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
        "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
        "- Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
        "- Исключим стоп-слова с помощью stop_words='english'.\n",
        "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcrQuVa0A2fR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amAEGWaWA3cX"
      },
      "source": [
        "## Задание 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz4V03sgBf9u"
      },
      "source": [
        "Натренируем gensim.models.Word2Vec модель на наших данных.\n",
        "- Тренировать будем на токенизированных твитах combine_df['tweet_token']\n",
        "- Установим следующие параметры: size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34.\n",
        "- Используем функцию train() с параметром total_examples равным длине combine_df['tweet_token'], количество epochs установим 20.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixXIsZiTA4m8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfrfZyG1A5aM"
      },
      "source": [
        "## Задание 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XxmsZQHBrmD"
      },
      "source": [
        "Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово positive = \"dinner\", и модель вытащит из корпуса наиболее похожие слова c помощью функции most_similar. То же самое попробуем со словом \"trump\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhKibRa8A6f4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT-9RZbCA7I8"
      },
      "source": [
        "## Задание 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2qWvKlYCIHs"
      },
      "source": [
        "Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова).\n",
        "<br>Давайте проверим векторное представление любого слова из нашего корпуса, например \"food\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC_Vc5V2A83D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbTpdnrNA9mN"
      },
      "source": [
        "## Задание 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yd_wQbkCUS1"
      },
      "source": [
        "Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите. Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. Теперь у нас есть 200 функций word2vec для наших данных.\n",
        "<br>Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите. В цикле сделать:  vec += model_w2v[word].reshape((1, size)) и поделить финальный вектор на количество слов в твите. На выходе должен получиться wordvec_df.shape = (49159, 200).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4dYrxHIA-fL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}