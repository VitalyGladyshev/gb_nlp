{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsa6XdW8/rcaFIpsYTqMci",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VitalyGladyshev/gb_nlp/blob/main/HW_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc-e3zAz75Nb"
      },
      "source": [
        "# ДЗ 02"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDhhr_EdDCQ9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEjzCbVxGJ_5",
        "outputId": "a94a39d9-4e08-4ec5-fe97-a33d272e3a4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxdk_PfPGR9_"
      },
      "source": [
        "path_nlp = \"/content/gdrive/My Drive/Colab Notebooks/NLP/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtLG-QX4FeYG"
      },
      "source": [
        "train_cor = pd.read_pickle(path_nlp + \"train_cor.pkl\")\n",
        "test_cor = pd.read_pickle(path_nlp + \"test_cor.pkl\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-cHtRE-Gr_o",
        "outputId": "030f4e57-3506-46f9-886e-1ee96cc7a12c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "train_cor.tail(2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>clean_tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>tweet_token_filtered</th>\n",
              "      <th>tweet_stemmed</th>\n",
              "      <th>tweet_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31960</th>\n",
              "      <td>31961</td>\n",
              "      <td>1</td>\n",
              "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
              "      <td>sikh temple vandalised in in calgary wso conde...</td>\n",
              "      <td>[sikh, temple, vandalised, in, in, calgary, ws...</td>\n",
              "      <td>[sikh, temple, vandalised, calgary, wso, conde...</td>\n",
              "      <td>{act, calgari, sikh, wso, condemn, templ, vand...</td>\n",
              "      <td>{act, vandalise, sikh, wso, temple, condemn, c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31961</th>\n",
              "      <td>31962</td>\n",
              "      <td>0</td>\n",
              "      <td>thank you @user for you follow</td>\n",
              "      <td>thank you for you follow</td>\n",
              "      <td>[thank, you, for, you, follow]</td>\n",
              "      <td>[thank, follow]</td>\n",
              "      <td>{thank, follow}</td>\n",
              "      <td>{thank, follow}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                   tweet_lemmatized\n",
              "31960  31961  ...  {act, vandalise, sikh, wso, temple, condemn, c...\n",
              "31961  31962  ...                                    {thank, follow}\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2qwUjUGxLK",
        "outputId": "b558e956-46ca-481c-851f-beb1813c81c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "test_cor.tail(2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>clean_tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>tweet_token_filtered</th>\n",
              "      <th>tweet_stemmed</th>\n",
              "      <th>tweet_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49157</th>\n",
              "      <td>49158</td>\n",
              "      <td>happy, at work conference: right mindset leads...</td>\n",
              "      <td>happy at work conference right mindset leads t...</td>\n",
              "      <td>[happy, at, work, conference, right, mindset, ...</td>\n",
              "      <td>[happy, work, conference, right, mindset, lead...</td>\n",
              "      <td>{cultureofdevelop, right, confer, lead, mindse...</td>\n",
              "      <td>{conference, right, happy, cultureofdevelopmen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49158</th>\n",
              "      <td>49159</td>\n",
              "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
              "      <td>my song so glad free download shoegaze newmusi...</td>\n",
              "      <td>[my, song, so, glad, free, download, shoegaze,...</td>\n",
              "      <td>[song, glad, free, download, shoegaze, newmusi...</td>\n",
              "      <td>{free, newmus, song, glad, download, newsong, ...</td>\n",
              "      <td>{free, song, glad, newmusic, download, shoegaz...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                   tweet_lemmatized\n",
              "49157  49158  ...  {conference, right, happy, cultureofdevelopmen...\n",
              "49158  49159  ...  {free, song, glad, newmusic, download, shoegaz...\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j34p8UCYKVpJ"
      },
      "source": [
        "test_cor['label'] = -1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUNiAYnUKbz5",
        "outputId": "b3abc926-76ec-46f0-f407-d845211b9f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "data = train_cor.copy()\n",
        "data = data.append(test_cor, ignore_index=True)\n",
        "data.tail(2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>clean_tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>tweet_token_filtered</th>\n",
              "      <th>tweet_stemmed</th>\n",
              "      <th>tweet_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49157</th>\n",
              "      <td>49158</td>\n",
              "      <td>-1</td>\n",
              "      <td>happy, at work conference: right mindset leads...</td>\n",
              "      <td>happy at work conference right mindset leads t...</td>\n",
              "      <td>[happy, at, work, conference, right, mindset, ...</td>\n",
              "      <td>[happy, work, conference, right, mindset, lead...</td>\n",
              "      <td>{cultureofdevelop, right, confer, lead, mindse...</td>\n",
              "      <td>{conference, right, happy, cultureofdevelopmen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49158</th>\n",
              "      <td>49159</td>\n",
              "      <td>-1</td>\n",
              "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
              "      <td>my song so glad free download shoegaze newmusi...</td>\n",
              "      <td>[my, song, so, glad, free, download, shoegaze,...</td>\n",
              "      <td>[song, glad, free, download, shoegaze, newmusi...</td>\n",
              "      <td>{free, newmus, song, glad, download, newsong, ...</td>\n",
              "      <td>{free, song, glad, newmusic, download, shoegaz...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                   tweet_lemmatized\n",
              "49157  49158  ...  {conference, right, happy, cultureofdevelopmen...\n",
              "49158  49159  ...  {free, song, glad, newmusic, download, shoegaz...\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBYrXBu1AtTP"
      },
      "source": [
        "## Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p6I9WjABESp"
      },
      "source": [
        "Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
        "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
        "- Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
        "- Исключим стоп-слова с помощью stop_words='english'.\n",
        "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names().\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQS2EMSf7ysg"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0abyqVeRPI4Y"
      },
      "source": [
        "count_vectorizer_stemmed = CountVectorizer(stop_words='english', max_df=0.9, max_features=1000)\n",
        "count_vectorizer_lemmatized = CountVectorizer(stop_words='english', max_df=0.9, max_features=1000)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uT8wmEPQ2ZM",
        "outputId": "47c44c31-8490-449d-b1fe-e13b2fea5c33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "list(map(lambda x: \" \".join(x), data['tweet_stemmed'].values[:10]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kid drag father selfish dysfunct run',\n",
              " 'getthank thank offer caus use pdx van lyft disapoint wheelchair credit',\n",
              " 'majesti bihday',\n",
              " 'love model ur time take',\n",
              " 'factsguid societi motiv',\n",
              " 'pay get leav fare disput allshowandnogo chao talk big fan huge',\n",
              " 'danni camp tomorrow',\n",
              " 'school year next think revolutionschool girl exam imagin hate actorslif',\n",
              " 'cav love allin cleveland land champion clevelandcavali',\n",
              " 'gr welcom']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSEwKoK3OpXH"
      },
      "source": [
        "bag_of_words_stemmed = count_vectorizer_stemmed.fit_transform(list(map(lambda x: \" \".join(x), data['tweet_stemmed'].values)))\n",
        "bag_of_words_lemmatized = count_vectorizer_lemmatized.fit_transform(list(map(lambda x: \" \".join(x), data['tweet_lemmatized'].values)))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQI1xbufOpm1",
        "outputId": "dc2c7a01-4910-4cef-d4c9-30b16f7c7980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "bag_of_words_stemmed"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<49159x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 202636 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF0p-c2jTSSh",
        "outputId": "6465e469-3a96-4190-fb01-93a8012b8581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "bag_of_words_lemmatized"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<49159x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 191072 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK2c9sHNeeOb",
        "outputId": "34de71ec-3e21-47a5-df7e-29aa45e5fcaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "df_count_vect_stemmed = pd.DataFrame(bag_of_words_stemmed.toarray(), columns=count_vectorizer_stemmed.get_feature_names())\n",
        "df_count_vect_stemmed.head(2)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abl</th>\n",
              "      <th>absolut</th>\n",
              "      <th>accept</th>\n",
              "      <th>account</th>\n",
              "      <th>act</th>\n",
              "      <th>action</th>\n",
              "      <th>actor</th>\n",
              "      <th>actual</th>\n",
              "      <th>ad</th>\n",
              "      <th>adapt</th>\n",
              "      <th>add</th>\n",
              "      <th>adventur</th>\n",
              "      <th>affirm</th>\n",
              "      <th>afternoon</th>\n",
              "      <th>age</th>\n",
              "      <th>ago</th>\n",
              "      <th>agre</th>\n",
              "      <th>ahead</th>\n",
              "      <th>aist</th>\n",
              "      <th>album</th>\n",
              "      <th>aliv</th>\n",
              "      <th>allahsoil</th>\n",
              "      <th>allow</th>\n",
              "      <th>alon</th>\n",
              "      <th>alreadi</th>\n",
              "      <th>altwaystoh</th>\n",
              "      <th>alway</th>\n",
              "      <th>amaz</th>\n",
              "      <th>america</th>\n",
              "      <th>american</th>\n",
              "      <th>amp</th>\n",
              "      <th>angel</th>\n",
              "      <th>anger</th>\n",
              "      <th>angri</th>\n",
              "      <th>anim</th>\n",
              "      <th>anniversari</th>\n",
              "      <th>announc</th>\n",
              "      <th>anoth</th>\n",
              "      <th>answer</th>\n",
              "      <th>anxieti</th>\n",
              "      <th>...</th>\n",
              "      <th>went</th>\n",
              "      <th>wet</th>\n",
              "      <th>whatev</th>\n",
              "      <th>white</th>\n",
              "      <th>wife</th>\n",
              "      <th>wild</th>\n",
              "      <th>win</th>\n",
              "      <th>wine</th>\n",
              "      <th>winner</th>\n",
              "      <th>wish</th>\n",
              "      <th>woh</th>\n",
              "      <th>woman</th>\n",
              "      <th>women</th>\n",
              "      <th>wonder</th>\n",
              "      <th>word</th>\n",
              "      <th>work</th>\n",
              "      <th>workout</th>\n",
              "      <th>world</th>\n",
              "      <th>worri</th>\n",
              "      <th>worst</th>\n",
              "      <th>wow</th>\n",
              "      <th>write</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wtf</th>\n",
              "      <th>xx</th>\n",
              "      <th>xxx</th>\n",
              "      <th>ya</th>\n",
              "      <th>yay</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yo</th>\n",
              "      <th>yoga</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>youtub</th>\n",
              "      <th>yr</th>\n",
              "      <th>yrs</th>\n",
              "      <th>yummi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   abl  absolut  accept  account  act  ...  young  youtub  yr  yrs  yummi\n",
              "0    0        0       0        0    0  ...      0       0   0    0      0\n",
              "1    0        0       0        0    0  ...      0       0   0    0      0\n",
              "\n",
              "[2 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GoyrXe2e6_A",
        "outputId": "897220a2-a29d-4dd8-919c-b621aecc9472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "df_count_vect_lemmatized = pd.DataFrame(bag_of_words_lemmatized.toarray(), columns=count_vectorizer_lemmatized.get_feature_names())\n",
        "df_count_vect_lemmatized.head(2)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>able</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>accept</th>\n",
              "      <th>account</th>\n",
              "      <th>act</th>\n",
              "      <th>action</th>\n",
              "      <th>actor</th>\n",
              "      <th>actually</th>\n",
              "      <th>adapt</th>\n",
              "      <th>add</th>\n",
              "      <th>adventure</th>\n",
              "      <th>advice</th>\n",
              "      <th>affirmation</th>\n",
              "      <th>affirmations</th>\n",
              "      <th>afternoon</th>\n",
              "      <th>age</th>\n",
              "      <th>ago</th>\n",
              "      <th>agree</th>\n",
              "      <th>ahead</th>\n",
              "      <th>aist</th>\n",
              "      <th>album</th>\n",
              "      <th>alive</th>\n",
              "      <th>allahsoil</th>\n",
              "      <th>alligator</th>\n",
              "      <th>allow</th>\n",
              "      <th>altwaystoheal</th>\n",
              "      <th>amaze</th>\n",
              "      <th>america</th>\n",
              "      <th>american</th>\n",
              "      <th>americans</th>\n",
              "      <th>amp</th>\n",
              "      <th>anger</th>\n",
              "      <th>angry</th>\n",
              "      <th>animals</th>\n",
              "      <th>anniversary</th>\n",
              "      <th>announce</th>\n",
              "      <th>answer</th>\n",
              "      <th>anxiety</th>\n",
              "      <th>anymore</th>\n",
              "      <th>app</th>\n",
              "      <th>...</th>\n",
              "      <th>white</th>\n",
              "      <th>wife</th>\n",
              "      <th>wild</th>\n",
              "      <th>win</th>\n",
              "      <th>wine</th>\n",
              "      <th>winner</th>\n",
              "      <th>wish</th>\n",
              "      <th>woh</th>\n",
              "      <th>woman</th>\n",
              "      <th>women</th>\n",
              "      <th>wonder</th>\n",
              "      <th>wonderful</th>\n",
              "      <th>word</th>\n",
              "      <th>work</th>\n",
              "      <th>workout</th>\n",
              "      <th>world</th>\n",
              "      <th>worry</th>\n",
              "      <th>worse</th>\n",
              "      <th>worst</th>\n",
              "      <th>wow</th>\n",
              "      <th>write</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wtf</th>\n",
              "      <th>xx</th>\n",
              "      <th>xxx</th>\n",
              "      <th>ya</th>\n",
              "      <th>yay</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yo</th>\n",
              "      <th>yoga</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>youtube</th>\n",
              "      <th>yr</th>\n",
              "      <th>yrs</th>\n",
              "      <th>yummy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   able  absolutely  accept  account  act  ...  young  youtube  yr  yrs  yummy\n",
              "0     0           0       0        0    0  ...      0        0   0    0      0\n",
              "1     0           0       0        0    0  ...      0        0   0    0      0\n",
              "\n",
              "[2 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FNwHKswAzxO"
      },
      "source": [
        "## Задание 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGSkgAooBX-G"
      },
      "source": [
        "Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
        "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
        "- Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
        "- Исключим стоп-слова с помощью stop_words='english'.\n",
        "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcrQuVa0A2fR"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4estmR5_c_Bi"
      },
      "source": [
        "tfidf_vectorizer_stemmed = TfidfVectorizer(stop_words='english', max_df=0.9, max_features=1000)\n",
        "tfidf_vectorizer_lemmatized = TfidfVectorizer(stop_words='english', max_df=0.9, max_features=1000)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJR1k5ixc_GN"
      },
      "source": [
        "bag_of_words_tfidf_stemmed = tfidf_vectorizer_stemmed.fit_transform(list(map(lambda x: \" \".join(x), data['tweet_stemmed'].values)))\n",
        "bag_of_words_tfidf_lemmatized = tfidf_vectorizer_lemmatized.fit_transform(list(map(lambda x: \" \".join(x), data['tweet_lemmatized'].values)))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuMMepApeHj1",
        "outputId": "efa1b0f0-8dfa-40c9-ac84-cb2e59a9fd19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "bag_of_words_tfidf_stemmed"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<49159x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 202636 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Irfp4ZheI5a",
        "outputId": "a0b6c837-118e-4f43-c36b-3b71857ec9aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "bag_of_words_tfidf_lemmatized"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<49159x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 191072 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWbHfyNAf3uI",
        "outputId": "0fbcc75c-5611-4571-e76c-cf3a80c035d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "df_tfidf_vect_stemmed = pd.DataFrame(bag_of_words_tfidf_stemmed.toarray(), columns=tfidf_vectorizer_stemmed.get_feature_names())\n",
        "df_tfidf_vect_stemmed.head(2)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abl</th>\n",
              "      <th>absolut</th>\n",
              "      <th>accept</th>\n",
              "      <th>account</th>\n",
              "      <th>act</th>\n",
              "      <th>action</th>\n",
              "      <th>actor</th>\n",
              "      <th>actual</th>\n",
              "      <th>ad</th>\n",
              "      <th>adapt</th>\n",
              "      <th>add</th>\n",
              "      <th>adventur</th>\n",
              "      <th>affirm</th>\n",
              "      <th>afternoon</th>\n",
              "      <th>age</th>\n",
              "      <th>ago</th>\n",
              "      <th>agre</th>\n",
              "      <th>ahead</th>\n",
              "      <th>aist</th>\n",
              "      <th>album</th>\n",
              "      <th>aliv</th>\n",
              "      <th>allahsoil</th>\n",
              "      <th>allow</th>\n",
              "      <th>alon</th>\n",
              "      <th>alreadi</th>\n",
              "      <th>altwaystoh</th>\n",
              "      <th>alway</th>\n",
              "      <th>amaz</th>\n",
              "      <th>america</th>\n",
              "      <th>american</th>\n",
              "      <th>amp</th>\n",
              "      <th>angel</th>\n",
              "      <th>anger</th>\n",
              "      <th>angri</th>\n",
              "      <th>anim</th>\n",
              "      <th>anniversari</th>\n",
              "      <th>announc</th>\n",
              "      <th>anoth</th>\n",
              "      <th>answer</th>\n",
              "      <th>anxieti</th>\n",
              "      <th>...</th>\n",
              "      <th>went</th>\n",
              "      <th>wet</th>\n",
              "      <th>whatev</th>\n",
              "      <th>white</th>\n",
              "      <th>wife</th>\n",
              "      <th>wild</th>\n",
              "      <th>win</th>\n",
              "      <th>wine</th>\n",
              "      <th>winner</th>\n",
              "      <th>wish</th>\n",
              "      <th>woh</th>\n",
              "      <th>woman</th>\n",
              "      <th>women</th>\n",
              "      <th>wonder</th>\n",
              "      <th>word</th>\n",
              "      <th>work</th>\n",
              "      <th>workout</th>\n",
              "      <th>world</th>\n",
              "      <th>worri</th>\n",
              "      <th>worst</th>\n",
              "      <th>wow</th>\n",
              "      <th>write</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wtf</th>\n",
              "      <th>xx</th>\n",
              "      <th>xxx</th>\n",
              "      <th>ya</th>\n",
              "      <th>yay</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yo</th>\n",
              "      <th>yoga</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>youtub</th>\n",
              "      <th>yr</th>\n",
              "      <th>yrs</th>\n",
              "      <th>yummi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   abl  absolut  accept  account  act  ...  young  youtub   yr  yrs  yummi\n",
              "0  0.0      0.0     0.0      0.0  0.0  ...    0.0     0.0  0.0  0.0    0.0\n",
              "1  0.0      0.0     0.0      0.0  0.0  ...    0.0     0.0  0.0  0.0    0.0\n",
              "\n",
              "[2 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8pJlxqkf3pN",
        "outputId": "5bcdde9e-0eda-4c1b-cfa6-47324158c3a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "df_tfidf_vect_stemmed = pd.DataFrame(bag_of_words_tfidf_lemmatized.toarray(), columns=tfidf_vectorizer_lemmatized.get_feature_names())\n",
        "df_tfidf_vect_stemmed.head(2)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>able</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>accept</th>\n",
              "      <th>account</th>\n",
              "      <th>act</th>\n",
              "      <th>action</th>\n",
              "      <th>actor</th>\n",
              "      <th>actually</th>\n",
              "      <th>adapt</th>\n",
              "      <th>add</th>\n",
              "      <th>adventure</th>\n",
              "      <th>advice</th>\n",
              "      <th>affirmation</th>\n",
              "      <th>affirmations</th>\n",
              "      <th>afternoon</th>\n",
              "      <th>age</th>\n",
              "      <th>ago</th>\n",
              "      <th>agree</th>\n",
              "      <th>ahead</th>\n",
              "      <th>aist</th>\n",
              "      <th>album</th>\n",
              "      <th>alive</th>\n",
              "      <th>allahsoil</th>\n",
              "      <th>alligator</th>\n",
              "      <th>allow</th>\n",
              "      <th>altwaystoheal</th>\n",
              "      <th>amaze</th>\n",
              "      <th>america</th>\n",
              "      <th>american</th>\n",
              "      <th>americans</th>\n",
              "      <th>amp</th>\n",
              "      <th>anger</th>\n",
              "      <th>angry</th>\n",
              "      <th>animals</th>\n",
              "      <th>anniversary</th>\n",
              "      <th>announce</th>\n",
              "      <th>answer</th>\n",
              "      <th>anxiety</th>\n",
              "      <th>anymore</th>\n",
              "      <th>app</th>\n",
              "      <th>...</th>\n",
              "      <th>white</th>\n",
              "      <th>wife</th>\n",
              "      <th>wild</th>\n",
              "      <th>win</th>\n",
              "      <th>wine</th>\n",
              "      <th>winner</th>\n",
              "      <th>wish</th>\n",
              "      <th>woh</th>\n",
              "      <th>woman</th>\n",
              "      <th>women</th>\n",
              "      <th>wonder</th>\n",
              "      <th>wonderful</th>\n",
              "      <th>word</th>\n",
              "      <th>work</th>\n",
              "      <th>workout</th>\n",
              "      <th>world</th>\n",
              "      <th>worry</th>\n",
              "      <th>worse</th>\n",
              "      <th>worst</th>\n",
              "      <th>wow</th>\n",
              "      <th>write</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wtf</th>\n",
              "      <th>xx</th>\n",
              "      <th>xxx</th>\n",
              "      <th>ya</th>\n",
              "      <th>yay</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yo</th>\n",
              "      <th>yoga</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>youtube</th>\n",
              "      <th>yr</th>\n",
              "      <th>yrs</th>\n",
              "      <th>yummy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   able  absolutely  accept  account  act  ...  young  youtube   yr  yrs  yummy\n",
              "0   0.0         0.0     0.0      0.0  0.0  ...    0.0      0.0  0.0  0.0    0.0\n",
              "1   0.0         0.0     0.0      0.0  0.0  ...    0.0      0.0  0.0  0.0    0.0\n",
              "\n",
              "[2 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amAEGWaWA3cX"
      },
      "source": [
        "## Задание 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz4V03sgBf9u"
      },
      "source": [
        "Натренируем gensim.models.Word2Vec модель на наших данных.\n",
        "- Тренировать будем на токенизированных твитах combine_df['tweet_token']\n",
        "- Установим следующие параметры: size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34.\n",
        "- Используем функцию train() с параметром total_examples равным длине combine_df['tweet_token'], количество epochs установим 20.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixXIsZiTA4m8"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73SEOcELmeCP"
      },
      "source": [
        "model_w2v = Word2Vec(size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU1C7juwqIY_"
      },
      "source": [
        "model_w2v.build_vocab(data['tweet_token'].values)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INwL80peqIUF",
        "outputId": "4a374f43-a40c-40d8-fc84-18ec33b2c8d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_w2v.train(data['tweet_token'].values, total_examples=len(data['tweet_token'].values), epochs=20, report_delay=1)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9033885, 11669180)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e38eyuOJmd81",
        "outputId": "cbf82295-64e6-4cd5-f12f-f19068cb2535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "model_w2v.wv.most_similar(positive=[\"car\"])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('truck', 0.5263153314590454),\n",
              " ('provo', 0.5135538578033447),\n",
              " ('caffine', 0.49682706594467163),\n",
              " ('crash', 0.49490049481391907),\n",
              " ('seat', 0.493663489818573),\n",
              " ('newadventure', 0.48260825872421265),\n",
              " ('glacier', 0.47989988327026367),\n",
              " ('accident', 0.47872379422187805),\n",
              " ('hsp', 0.47774893045425415),\n",
              " ('apament', 0.474631130695343)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfrfZyG1A5aM"
      },
      "source": [
        "## Задание 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XxmsZQHBrmD"
      },
      "source": [
        "Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово positive = \"dinner\", и модель вытащит из корпуса наиболее похожие слова c помощью функции most_similar. То же самое попробуем со словом \"trump\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhKibRa8A6f4",
        "outputId": "56be9191-6c0f-43ae-eb7c-4b3cea1082b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "model_w2v.wv.most_similar(positive=[\"dinner\"])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spaghetti', 0.5896735191345215),\n",
              " ('cookout', 0.5851148962974548),\n",
              " ('bihdaydinner', 0.5799993872642517),\n",
              " ('tacotuesday', 0.5735299587249756),\n",
              " ('saturdate', 0.5697401762008667),\n",
              " ('lunchtime', 0.5667805671691895),\n",
              " ('bolognese', 0.5645062923431396),\n",
              " ('lastnight', 0.5589499473571777),\n",
              " ('lamb', 0.5585891604423523),\n",
              " ('iftar', 0.5579138398170471)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBBHMcdKtXFP",
        "outputId": "964df8ae-0faf-4aba-e7f6-22c11d86e0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "model_w2v.wv.most_similar(positive=[\"trump\"])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('donald', 0.5525873899459839),\n",
              " ('dumptrump', 0.5486760139465332),\n",
              " ('conman', 0.5186482071876526),\n",
              " ('suppoer', 0.5182057619094849),\n",
              " ('potus', 0.5109132528305054),\n",
              " ('trumptrain', 0.5054063200950623),\n",
              " ('donaldtrump', 0.5050008296966553),\n",
              " ('cochairman', 0.5049580335617065),\n",
              " ('presidentelect', 0.500296413898468),\n",
              " ('paladino', 0.4999353587627411)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT-9RZbCA7I8"
      },
      "source": [
        "## Задание 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2qWvKlYCIHs"
      },
      "source": [
        "Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова).\n",
        "<br>Давайте проверим векторное представление любого слова из нашего корпуса, например \"food\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC_Vc5V2A83D",
        "outputId": "c39b42ce-dd42-4943-f5f9-18647f7b796a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "model_w2v.wv.word_vec(\"food\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-6.11505330e-01, -7.13534892e-01, -5.10297157e-02,  7.61005938e-01,\n",
              "       -1.00077820e+00,  5.00927009e-02, -1.21215510e+00,  1.73618659e-01,\n",
              "       -2.33678058e-01,  2.01332748e-01, -3.61983508e-01,  3.04014325e-01,\n",
              "       -1.93961218e-01,  5.02443433e-01, -3.70827228e-01, -1.10314772e-01,\n",
              "       -3.23230863e-01, -3.05364072e-01,  4.38812673e-01, -2.34569773e-01,\n",
              "        1.43866271e-01,  6.59991920e-01, -3.37127447e-01, -4.44977403e-01,\n",
              "        1.99118242e-01,  1.08627312e-01, -6.68186620e-02,  5.45131445e-01,\n",
              "        3.95526201e-01, -1.76000565e-01,  2.09798083e-01, -4.15841073e-01,\n",
              "        2.62166560e-01, -5.41864753e-01, -2.80287206e-01,  2.84253418e-01,\n",
              "        5.43373585e-01,  3.89108770e-02,  5.21777391e-01, -2.71133363e-01,\n",
              "       -2.64734812e-02, -4.01863128e-01,  8.35564196e-01, -1.51192263e-01,\n",
              "        5.66735029e-01, -1.35796562e-01,  3.89534652e-01,  1.20460801e-01,\n",
              "        1.97285458e-01, -6.76709354e-01,  7.62677118e-02,  3.73187631e-01,\n",
              "       -3.99832847e-03,  3.02311368e-02, -1.35645280e-02,  3.24912786e-01,\n",
              "       -6.28643036e-01, -1.01422870e+00,  3.43552232e-02, -1.29757673e-01,\n",
              "       -7.74587840e-02, -6.30949795e-01, -5.76836288e-01,  5.76931715e-01,\n",
              "        1.17562127e+00,  6.79952025e-01,  5.86210191e-03,  2.94752326e-02,\n",
              "       -2.07295030e-01, -5.16441464e-01, -3.99335414e-01, -6.02464616e-01,\n",
              "       -1.38235733e-01,  4.21686977e-01,  5.90093553e-01,  3.86565238e-01,\n",
              "       -5.06422758e-01, -3.04550201e-01, -8.35759044e-01,  6.53995201e-02,\n",
              "       -2.42452428e-01,  7.81990767e-01, -3.08974326e-01,  1.82478473e-01,\n",
              "        2.94632792e-01,  6.10323343e-03, -9.56577733e-02, -4.65053581e-02,\n",
              "        6.71711043e-02,  6.01073563e-01,  3.81607711e-01,  7.17664838e-01,\n",
              "        9.40269965e-04, -1.89873993e-01, -3.02336305e-01,  3.48607510e-01,\n",
              "       -2.04324633e-01,  4.02299911e-01, -6.18915796e-01,  3.29001009e-01,\n",
              "       -3.51342171e-01, -7.18247220e-02, -1.36224896e-01, -8.29954818e-02,\n",
              "        2.39722848e-01,  4.62502893e-03, -3.71391863e-01,  5.43409646e-01,\n",
              "        2.84687579e-01, -4.86216605e-01, -3.26418966e-01, -5.35081998e-02,\n",
              "       -2.34164715e-01, -1.80973500e-01,  1.23852879e-01,  3.49811763e-01,\n",
              "       -1.51516169e-01, -4.70468365e-02,  4.01027113e-01, -2.64725447e-01,\n",
              "       -4.50994164e-01, -4.47674900e-01, -1.15600921e-01, -1.04914939e+00,\n",
              "       -2.12245196e-01,  5.99919736e-01,  7.12028444e-01,  2.42104754e-01,\n",
              "       -1.18967867e+00, -4.72190678e-01,  4.52834442e-02,  1.92603022e-01,\n",
              "        2.80357718e-01, -2.17112988e-01,  1.61305681e-01, -6.04034066e-01,\n",
              "        3.36873204e-01,  2.77409256e-01,  1.09859735e-01,  5.72536528e-01,\n",
              "       -4.69593890e-02,  1.85122907e-01,  1.31006702e-03, -3.79624307e-01,\n",
              "       -3.27244520e-01,  2.87752748e-01, -2.36792117e-01, -6.32507026e-01,\n",
              "       -1.42762601e-01, -1.42565653e-01,  1.36265874e-01, -2.96842992e-01,\n",
              "       -3.40289444e-01,  3.64734270e-02,  3.12857538e-01, -2.29913387e-02,\n",
              "        1.40255854e-01,  2.24826127e-01,  7.29824126e-01, -1.54316694e-01,\n",
              "        7.24677742e-02,  4.15779911e-02, -4.81917918e-01, -8.47829655e-02,\n",
              "       -3.26458007e-01,  1.14956245e-01,  5.56404412e-01,  4.11197275e-01,\n",
              "       -2.10792914e-01,  2.31128901e-01,  4.40142572e-01, -6.30977690e-01,\n",
              "       -2.93835670e-01, -5.50582290e-01, -6.14914119e-01, -1.62033632e-01,\n",
              "        3.97824377e-01,  1.81935892e-01,  4.96712118e-01,  9.76404771e-02,\n",
              "       -2.50896066e-01, -1.34473413e-01,  4.90246594e-01, -1.52827695e-01,\n",
              "        1.67529568e-01,  2.83048540e-01, -3.08338434e-01, -2.37818509e-01,\n",
              "        2.98425794e-01, -1.98090136e-01, -1.79571509e-01, -4.10644829e-01,\n",
              "       -1.95029542e-01,  3.55985612e-02,  5.53763509e-01,  5.32955766e-01,\n",
              "        4.57620323e-01, -3.52989197e-01,  2.16144070e-01,  4.04527575e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbTpdnrNA9mN"
      },
      "source": [
        "## Задание 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yd_wQbkCUS1"
      },
      "source": [
        "Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите. Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. Теперь у нас есть 200 функций word2vec для наших данных.\n",
        "<br>Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите. В цикле сделать:  vec += model_w2v[word].reshape((1, size)) и поделить финальный вектор на количество слов в твите. На выходе должен получиться wordvec_df.shape = (49159, 200).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygFYq5Rz4DnH"
      },
      "source": [
        "from typing import List"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnmOuI7U2ioW"
      },
      "source": [
        "def w2v_mean(data: pd.Series)->List:\n",
        "    wordvec_df = []\n",
        "    size = 200\n",
        "    for sent in data.values:\n",
        "        vec = 0\n",
        "        cnt = 0\n",
        "        for word in sent:\n",
        "            try:\n",
        "                vec += model_w2v[word].reshape((1, size))\n",
        "                cnt +=1\n",
        "            except KeyError as err:\n",
        "                pass\n",
        "                # print(f\"Exception type {type(err)}: {err}\")\n",
        "        if cnt:\n",
        "            vec /= cnt\n",
        "        wordvec_df.append(vec)\n",
        "    return wordvec_df"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4dYrxHIA-fL",
        "outputId": "5e49ea36-d8dd-4a10-ee93-744e9d96a982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "res = w2v_mean(data['tweet_token'])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwm5AfnRyydR",
        "outputId": "39e28ee7-2825-4300-de0a-6a44dc374aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(res)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49159"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oozKKrwO4oBK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}